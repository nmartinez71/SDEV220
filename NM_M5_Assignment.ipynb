{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Module 5 Assignment \n",
    "Neftali Martinez \n",
<<<<<<< HEAD
    "This program is the following of a tutorial of Testing\n",
=======
    "This program is the Testing Assignment.\n",
>>>>>>> 872422c1d5ba631afa9f8c49f056f92290af0dad
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 25,
>>>>>>> 872422c1d5ba631afa9f8c49f056f92290af0dad
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "usage: ipykernel_launcher.py [-h] [-v] [-q] [--locals] [-f] [-c] [-b]\n",
      "                             [-k TESTNAMEPATTERNS]\n",
      "                             [tests ...]\n",
      "ipykernel_launcher.py: error: argument -f/--failfast: ignored explicit argument 'c:\\\\Users\\\\exodu\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-v354379ad65d201bd3086a70a45c57fc2ea34a1239.json'\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
=======
      "test_list_fraction (__main__.TestSum.test_list_fraction)\n",
      "Test that it can sum a list of fractions ... FAIL\n",
      "test_list_int (__main__.TestSum.test_list_int)\n",
      "Test that it can sum a list of integers ... ok\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_list_fraction (__main__.TestSum.test_list_fraction)\n",
      "Test that it can sum a list of fractions\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\exodu\\AppData\\Local\\Temp\\ipykernel_4468\\2180292892.py\", line 20, in test_list_fraction\n",
      "    self.assertEqual(result, 1)\n",
      "    ~~~~~~~~~~~~~~~~^^^^^^^^^^^\n",
      "AssertionError: Fraction(9, 10) != 1\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.001s\n",
      "\n",
      "FAILED (failures=1)\n"
>>>>>>> 872422c1d5ba631afa9f8c49f056f92290af0dad
     ]
    }
   ],
   "source": [
    "import unittest\n",
<<<<<<< HEAD
    "\n",
    "class TestSum(unittest.TestCase):\n",
    "    def test_sum(self):\n",
    "        self.assertEqual(sum([1, 2, 3]), 6, \"Should be 6\")\n",
    "    def test_sum_tuple(self):\n",
    "        self.assertEqual(sum((1, 2, 2)), 6, \"Should be 6\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
=======
    "from my_sum import sum\n",
    "from fractions import Fraction\n",
    "\n",
    "class TestSum(unittest.TestCase):\n",
    "    def test_list_int(self):\n",
    "        \"\"\"\n",
    "        Test that it can sum a list of integers\n",
    "        \"\"\"\n",
    "        data = [1, 2, 3]\n",
    "        result = sum(data)\n",
    "        self.assertEqual(result, 6)\n",
    "\n",
    "    def test_list_fraction(self):\n",
    "        \"\"\"\n",
    "        Test that it can sum a list of fractions\n",
    "        \"\"\"\n",
    "        data = [Fraction(1, 4), Fraction(1, 4), Fraction(2, 5)]\n",
    "        result = sum(data)\n",
    "        self.assertEqual(result, 1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "What the test results mean to me:\n",
    "\n",
    "    These tests serve as a proof of the logic and math of ones code. As the developer sets the expectation of result to the computer its up to the test code\n",
    "    to prove whether the code results in the desired outcome or not. This ensures that at the very least that the code written results in the desired outcome since the user decides \n",
    "    what the outcome should be for any particular case. In essence its a way check if a program has the expected results.\n",
    "\"\"\""
>>>>>>> 872422c1d5ba631afa9f8c49f056f92290af0dad
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.10.11"
=======
   "version": "3.13.0"
>>>>>>> 872422c1d5ba631afa9f8c49f056f92290af0dad
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
